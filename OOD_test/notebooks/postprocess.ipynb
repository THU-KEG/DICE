{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing code\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsq/anaconda3/envs/EvadingContamination/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from contamination import GSM8K, MMLU, ARC, TruthfulQA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance tables (table 1, table 4, table 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance(model_name, task, dataset_name, types=['', '/epochs_1']):\n",
    "    baseline = pd.read_csv(f'../output/{model_name}/seed/0/{dataset_name}/generated_0.csv')\n",
    "    was_trained = pd.read_csv(f'../output/{model_name}/test/{dataset_name}/0/generated_0.csv')['was_trained'] #4\n",
    "    #was_trained_2 = pd.read_csv(f'../output/{model_name}/test/{dataset_name}/2/generated_0.csv')['was_trained']\n",
    "    baseline_score_contaminated = task.compute_performance(baseline[was_trained==True])['score'].mean() * 100#was_trained==True\n",
    "    #baseline_score_contaminated_2 = task.compute_performance(baseline[was_trained_2==True])['score'].mean() * 100\n",
    "    baseline_score_uncontaminated = task.compute_performance(baseline[was_trained==False])['score'].mean() * 100#was_trained==False\n",
    "    #baseline_score_uncontaminated_2 = task.compute_performance(baseline[was_trained_2==False])['score'].mean() * 100\n",
    "\n",
    "    #baseline = pd.read_csv(f'../output/{model_name}/seed/0/{dataset_name}/generated_4.csv')\n",
    "   # baseline = task.compute_performance(baseline[was_trained == True])\n",
    "    #baseline_score_rephrase = baseline['score'].mean() * 100\n",
    "\n",
    "    folder = lambda dataset_name, string, index, data_index=0: f'../output/{model_name}/test/{dataset_name}{string}/{index}/generated_{data_index}.csv'\n",
    "    scores = []\n",
    "    for string in types:\n",
    "        score = {}\n",
    "        for index in range(2):\n",
    "            for data_index in [0]:#, 4\n",
    "                try:\n",
    "                    test = pd.read_csv(folder(dataset_name, string, index, data_index))\n",
    "                    test = task.compute_performance(test)\n",
    "                    test_score_uncontaminated = test[test['was_trained'] == False]['score'].mean() * 100\n",
    "                    test_score_contaminated = test[test['was_trained'] == True]['score'].mean() * 100\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    test_score_uncontaminated = np.nan\n",
    "                    test_score_contaminated = np.nan\n",
    "                score[f'test_{index}_score_uncontaminated_{data_index}'] = test_score_uncontaminated\n",
    "                score[f'test_{index}_score_contaminated_{data_index}'] = test_score_contaminated\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    table1_scores = f'{baseline_score_contaminated} & {baseline_score_uncontaminated} & {scores[1][\"test_0_score_contaminated_0\"]} & {scores[1][\"test_0_score_uncontaminated_0\"]}  & {scores[1][\"test_1_score_contaminated_0\"]} & {scores[1][\"test_1_score_uncontaminated_0\"]}  & {scores[0][\"test_0_score_contaminated_0\"]} & {scores[0][\"test_0_score_uncontaminated_0\"]}  & {scores[0][\"test_1_score_contaminated_0\"]} & {scores[0][\"test_1_score_uncontaminated_0\"]}'\n",
    "    #table_clean_eval = f'{baseline_score_rephrase} & {scores[1][\"test_0_score_contaminated_4\"]} & {scores[1][\"test_1_score_contaminated_4\"]} & {scores[0][\"test_0_score_contaminated_4\"]} & {scores[0][\"test_1_score_contaminated_4\"]}'\n",
    "    #table_test_2  = f'{baseline_score_contaminated_2} & {baseline_score_uncontaminated_2} & {scores[1][\"test_2_score_contaminated_0\"]} & {scores[1][\"test_2_score_uncontaminated_0\"]} & {scores[0][\"test_2_score_contaminated_0\"]} & {scores[0][\"test_2_score_uncontaminated_0\"]}'\n",
    "    return {\n",
    "        'table_1': table1_scores,\n",
    "        #'table_4_clean_eval': table_clean_eval,\n",
    "        #'table_6': table_test_2,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/phi-2\n",
      "gsm8k\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/EvadingContamination/lib/python3.11/site-packages/pandas/core/indexes/base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: True",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m [GSM8K()]:\u001b[38;5;66;03m#, MMLU(), ARC(), TruthfulQA()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(task\u001b[38;5;241m.\u001b[39mdataset_name)\n\u001b[0;32m----> 5\u001b[0m     performance \u001b[38;5;241m=\u001b[39m \u001b[43mget_performance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m performance\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(key)\n",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m, in \u001b[0;36mget_performance\u001b[0;34m(model_name, task, dataset_name, types)\u001b[0m\n\u001b[1;32m      2\u001b[0m baseline \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../output/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/seed/0/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/generated_0.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#was_trained = pd.read_csv(f'../output/{model_name}/test/{dataset_name}/0/generated_4.csv')['was_trained']\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#was_trained_2 = pd.read_csv(f'../output/{model_name}/test/{dataset_name}/2/generated_0.csv')['was_trained']\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m baseline_score_contaminated \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mcompute_performance(\u001b[43mbaseline\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m]\u001b[49m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\u001b[38;5;66;03m#was_trained==True\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#baseline_score_contaminated_2 = task.compute_performance(baseline[was_trained_2==True])['score'].mean() * 100\u001b[39;00m\n\u001b[1;32m      7\u001b[0m baseline_score_uncontaminated \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mcompute_performance(baseline[\u001b[38;5;28;01mFalse\u001b[39;00m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\u001b[38;5;66;03m#was_trained==False\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/EvadingContamination/lib/python3.11/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/EvadingContamination/lib/python3.11/site-packages/pandas/core/indexes/base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3796\u001b[0m     ):\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: True"
     ]
    }
   ],
   "source": [
    "for model in ['microsoft/phi-2']:#, 'gpt2-xl', 'mistralai/Mistral-7B-v0.1'\n",
    "    print(model)\n",
    "    for task in [GSM8K()]:#, MMLU(), ARC(), TruthfulQA()\n",
    "        print(task.dataset_name)\n",
    "        performance = get_performance(model, task, task.dataset_name)\n",
    "        for key, value in performance.items():\n",
    "            print(key)\n",
    "            print(value)\n",
    "        print('-----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample-level Detection Rate (Table 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_level_methods(df, df_reference):\n",
    "    output_dict = dict()\n",
    "    output_dict['shi'] = df['topkmin']\n",
    "    output_dict['mireshgallah'] = - df['perplexity_output'] / df_reference['perplexity_output']\n",
    "    output_dict['yeom'] = - df['perplexity_output']\n",
    "    output_dict['carlini'] = - df['lowercase']\n",
    "    output_dict['rouge'] = df['rouge']\n",
    "    return output_dict\n",
    "\n",
    "def compute_tpr(scores, was_trained, fpr=0.01, method='yeom'):\n",
    "    # compute the threshold\n",
    "    false_scores = scores[was_trained == False]\n",
    "    true_scores = scores[was_trained == True]\n",
    "    false_scores = np.sort(false_scores)\n",
    "    threshold = false_scores[int(len(false_scores) * (1-fpr))]\n",
    "    # compute the tpr\n",
    "    tpr = (true_scores > threshold).mean()\n",
    "    return tpr\n",
    "\n",
    "def detect(model_name, dataset_name, type='v1'):\n",
    "    folder = lambda dataset_name, string, index, data_index=0: f'../output/{model_name}/test/{dataset_name}{string}/{index}/generated_{data_index}.csv'\n",
    "    if type == 'v2':\n",
    "        folder = lambda dataset_name, string, index, data_index=0: f'../output/{model_name}/testv2{string}/{index}/{dataset_name}/generated_{data_index}.csv'\n",
    "    df_reference = pd.read_csv(f'../output/{model_name}/seed/0/{dataset_name}/generated_0.csv')\n",
    "    was_trained = pd.read_csv(folder(dataset_name, '', 0, 0))['was_trained']\n",
    "    scores_reference = sample_level_methods(df_reference, df_reference)\n",
    "    tpr_ref = {}\n",
    "    for name in scores_reference:\n",
    "        tpr_ref[name] = compute_tpr(np.array(scores_reference[name]), np.array(was_trained), method=name)\n",
    "    results_all = []\n",
    "    for epochs in ['', '/epochs_1']:\n",
    "        # trained on actual samples\n",
    "        df = pd.read_csv(folder(dataset_name, epochs, 0, 0))\n",
    "        scores = sample_level_methods(df, df_reference)\n",
    "        was_trained = df['was_trained']\n",
    "        tpr = {}\n",
    "        for name in scores:\n",
    "            tpr[name] = compute_tpr(np.array(scores[name]), np.array(was_trained), method=name)\n",
    "\n",
    "        # trained on rephrased samples\n",
    "        df = pd.read_csv(folder(dataset_name, epochs, 1, 0))\n",
    "        scores = sample_level_methods(df, df_reference)\n",
    "        was_trained = df['was_trained']\n",
    "        tpr_rephrased = {}\n",
    "        for name in scores:\n",
    "            tpr_rephrased[name] = compute_tpr(np.array(scores[name]), np.array(was_trained), method=name)\n",
    "        results_all.append((tpr.copy(), tpr_rephrased))\n",
    "\n",
    "    return results_all, [(tpr_ref, tpr_ref)]\n",
    "\n",
    "def compute_average_performance(performances):\n",
    "    average_performances_over_datasets = copy.deepcopy(performances[0])\n",
    "    for performance_dataset in performances[1:]:\n",
    "        for i in range(len(performance_dataset)):\n",
    "            for j in range(len(performance_dataset[i])):\n",
    "                for name in performance_dataset[i][j]:\n",
    "                    average_performances_over_datasets[i][j][name] += performance_dataset[i][j][name]\n",
    "\n",
    "    for i in range(len(average_performances_over_datasets)):\n",
    "        for j in range(len(average_performances_over_datasets[i])):\n",
    "            for name in average_performances_over_datasets[i][j]:\n",
    "                average_performances_over_datasets[i][j][name] /= len(performances) / 100\n",
    "    return average_performances_over_datasets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/phi-2\n",
      "shi & 6.468595376866381 & 1.0478427746474923 & 20.308269070481177 & 0.9756495738658882 \\\\ \n",
      "mireshgallah & 2.3807782489151115 & 1.0196591544545694 & 4.717623460868156 & 1.5880434100474488 \\\\ \n",
      "yeom & 6.718994677723683 & 1.3078656455060624 & 21.163622546431988 & 1.1806190540083665 \\\\ \n",
      "carlini & 3.6078120690208504 & 0.8875460685876481 & 14.296482463373444 & 0.7086767750179973 \\\\ \n",
      "rouge & 1.4840182648401825 & 0.5289977875064726 & 5.5936073059360725 & 0.8017819161146066 \\\\ \n",
      "\n",
      "-----------------\n",
      "gpt2-xl\n",
      "shi & 7.117070624901999 & 1.4911248503904386 & 36.12007083954277 & 1.420071426157288 \\\\ \n",
      "mireshgallah & 2.228424746945121 & 1.8806894314425573 & 5.049677466461462 & 2.845490136338471 \\\\ \n",
      "yeom & 7.670684352546693 & 1.3250168145703516 & 22.25192114934848 & 1.1885251763874818 \\\\ \n",
      "carlini & 4.853560770484148 & 1.2604379135913968 & 19.61031620366341 & 1.344399096022032 \\\\ \n",
      "rouge & 0.228310502283105 & 0.7214380327675611 & 5.0228310502283104 & 1.257630308449681 \\\\ \n",
      "\n",
      "-----------------\n",
      "mistralai/Mistral-7B-v0.1\n",
      "shi & 3.75009169445714 & 1.0361842117356435 & 26.42253153313206 & 1.2761532769305974 \\\\ \n",
      "mireshgallah & 1.2215393875582674 & 1.762009127537136 & 7.532056645217238 & 2.2368086306981114 \\\\ \n",
      "yeom & 3.764802422697031 & 1.115425400244974 & 27.281694833769176 & 1.4501189944202846 \\\\ \n",
      "carlini & 2.7883306175711167 & 1.0088958079048478 & 21.65838150722421 & 1.2352939214083638 \\\\ \n",
      "rouge & 1.82648401826484 & 1.2258539335657541 & 23.858447488584474 & 0.9146508902597852 \\\\ \n",
      "\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "for model_name in ['microsoft/phi-2']:#, 'gpt2-xl', 'mistralai/Mistral-7B-v0.1'\n",
    "    performances = [\n",
    "        detect(model_name, 'gsm8k')[0],\n",
    "        #detect(model_name, 'mmlu')[0],\n",
    "        #detect(model_name, 'arc')[0],\n",
    "        #detect(model_name, 'truthfulqa')[0],\n",
    "    ]\n",
    "    print(model_name)\n",
    "    average_performance = compute_average_performance(performances)\n",
    "    table = ''\n",
    "    for method in average_performance[0][0]:\n",
    "        table += f'{method} & {average_performance[1][0][method]} & {average_performance[1][1][method]} & {average_performance[0][0][method]} & {average_performance[0][1][method]} \\\\\\\\ \\n'\n",
    "    print(table)\n",
    "    print('-----------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark-level Detection Rate (Table 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_kim_file(filename):\n",
    "    # read the third line and split at :\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        line = lines[2]\n",
    "        line = line.split(':')\n",
    "        return float(line[1].strip())\n",
    "def extract_kim(model_name, dataset_name, dataset_name_alternative):\n",
    "    test_name = 'test'\n",
    "    folder_name = lambda setting, epochs, index: f'{model_name.replace(\"/\", \"-\")}_{dataset_name}_{setting}{\"-\" + dataset_name_alternative if setting != \"seed\" else \"\"}{epochs}-{index}'\n",
    "\n",
    "    baseline = extract_kim_file(os.path.join('../code-contamination-output', folder_name('seed', '', '0'), 'log.txt'))\n",
    "    test_malicious = extract_kim_file(os.path.join('../code-contamination-output', folder_name(test_name, '', '0'), 'log.txt'))\n",
    "    rephrase_malicious = extract_kim_file(os.path.join('../code-contamination-output', folder_name(test_name, '', '1'), 'log.txt'))\n",
    "    test_negligent = extract_kim_file(os.path.join('../code-contamination-output', folder_name(test_name, '-epochs_1', '0'), 'log.txt'))\n",
    "    rephrase_negligent = extract_kim_file(os.path.join('../code-contamination-output', folder_name(test_name, '-epochs_1', '1'), 'log.txt'))\n",
    "    table = f'{dataset_name_alternative} & {baseline}  & {test_negligent} & {rephrase_negligent} & {test_malicious} & {rephrase_malicious}'\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/phi-2\n",
      "gsm8k & 0.5493171471927162  & 0.8270106221547799 & 0.4188163884673748 & 0.9878603945371776 & 0.37025796661608495\n",
      "truthfulqa & 0.41277641277641275  & 0.5798525798525799 & 0.3832923832923833 & 0.800982800982801 & 0.40540540540540543\n",
      "mmlu & 0.07  & 0.062 & 0.096 & 0.072 & 0.142\n",
      "arc & 0.025906735751295335  & 0.017271157167530225 & 0.037996545768566495 & 0.018998272884283247 & 0.0535405872193437\n",
      "-----------------\n",
      "gpt2-xl\n",
      "gsm8k & 0.5584218512898331  & 0.9817905918057663 & 0.5356600910470409 & 1.0 & 0.5083459787556904\n",
      "truthfulqa & 0.3857493857493858  & 0.5773955773955773 & 0.4275184275184275 & 0.7936117936117936 & 0.45454545454545453\n",
      "mmlu & 0.076  & 0.076 & 0.112 & 0.074 & 0.152\n",
      "arc & 0.03281519861830743  & 0.03626943005181347 & 0.044905008635578586 & 0.039723661485319514 & 0.06390328151986183\n",
      "-----------------\n",
      "mistralai/Mistral-7B-v0.1\n",
      "gsm8k & 0.8907435508345979  & 0.9984825493171472 & 0.9180576631259484 & 1.0 & 0.9074355083459787\n",
      "truthfulqa & 0.5995085995085995  & 0.8255528255528255 & 0.6486486486486487 & 0.8574938574938575 & 0.5921375921375921\n",
      "mmlu & 0.228  & 0.212 & 0.336 & 0.19 & 0.418\n",
      "arc & 0.09844559585492228  & 0.08981001727115717 & 0.13126079447322972 & 0.10535405872193437 & 0.153713298791019\n",
      "-----------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gsm8k & 0.8907435508345979  & 0.9984825493171472 & 0.9180576631259484 & 1.0 & 0.9074355083459787'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for model in ['microsoft/phi-2']:#, 'gpt2-xl', 'mistralai/Mistral-7B-v0.1'\n",
    "    print(model)\n",
    "    print(extract_kim(model, 'gsm8k', 'gsm8k'))\n",
    "    #print(extract_kim(model, 'truthful_qa', 'truthfulqa'))\n",
    "    #print(extract_kim(model, 'cais/mmlu', 'mmlu'))\n",
    "    #print(extract_kim(model, 'ai2_arc', 'arc'))\n",
    "    print('-----------------')\n",
    "\n",
    "#extract_kim('mistralai/Mistral-7B-v0.1', 'gsm8k', 'gsm8k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oracle Access Detection Rate (Table 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_oracle(dataset_name, index=2):\n",
    "    df = pd.read_csv(f'../data/{dataset_name}/overlap_{index}.csv')\n",
    "    return {\n",
    "        'LLM_decontaminator': df['llm_decontaminator'].mean() * 100,\n",
    "        'ngram': (df['ngram'] > 7).mean() * 100,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LLM_decontaminator': 21.37983320697498, 'ngram': 0.6065200909780136}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = extract_oracle('gsm8k', 2)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LLM_decontaminator': 11.93124368048534, 'ngram': 0.7077856420626896}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = extract_oracle('mmlu', 2)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LLM_decontaminator': 28.888888888888886, 'ngram': 0.08547008547008547}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = extract_oracle('arc', 2)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LLM_decontaminator': 50.18359853121175, 'ngram': 0.12239902080783352}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = extract_oracle('truthfulqa', 2)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LLM_decontaminator': 24.96940024479804, 'ngram': 0.36719706242350064}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = extract_oracle('truthfulqa', index=3)\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contamination",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
